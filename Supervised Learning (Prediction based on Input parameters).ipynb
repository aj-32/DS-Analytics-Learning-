{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Mashable -- is a global, multi-platform media and entertainment\n",
    "company. Powered by its own proprietary technology, Mashable is the go-to source for tech,\n",
    "digital culture and entertainment content for its dedicated and influential audience around\n",
    "the globe.\n",
    "Just like any other media company its success depends on the popularity of articles. And one\n",
    "of the key metrics to measure popularity is no. of shares done on article.\n",
    "Over period of few years Mashable has collected data on around 40,000 articles.\n",
    "\n",
    "\n",
    "Objective : Performing analysis and modeling to predict number of shares of an article\n",
    "given the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('OnlineNewsPopularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                               0\n",
       "timedelta                         0\n",
       "n_tokens_title                    0\n",
       "n_tokens_content                  0\n",
       "n_unique_tokens                   0\n",
       "n_non_stop_words                  0\n",
       "n_non_stop_unique_tokens          0\n",
       "num_hrefs                         0\n",
       "num_self_hrefs                    0\n",
       "num_imgs                          0\n",
       "num_videos                        0\n",
       "average_token_length              0\n",
       "num_keywords                      0\n",
       "data_channel_is_lifestyle         0\n",
       " data_channel_is_entertainment    0\n",
       "data_channel_is_bus               0\n",
       "data_channel_is_socmed            0\n",
       "data_channel_is_tech              0\n",
       "data_channel_is_world             0\n",
       "kw_min_min                        0\n",
       "kw_max_min                        0\n",
       "kw_avg_min                        0\n",
       "kw_min_max                        0\n",
       "kw_max_max                        0\n",
       "kw_avg_max                        0\n",
       "kw_min_avg                        0\n",
       "kw_max_avg                        0\n",
       "kw_avg_avg                        0\n",
       "self_reference_min_shares         0\n",
       "self_reference_max_shares         0\n",
       "                                 ..\n",
       "weekday_is_monday                 0\n",
       "weekday_is_tuesday                0\n",
       "weekday_is_wednesday              0\n",
       "weekday_is_thursday               0\n",
       "weekday_is_friday                 0\n",
       "weekday_is_saturday               0\n",
       "weekday_is_sunday                 0\n",
       "is_weekend                        0\n",
       "LDA_00                            0\n",
       "LDA_01                            0\n",
       "LDA_02                            0\n",
       "LDA_03                            0\n",
       "LDA_04                            0\n",
       "global_subjectivity               0\n",
       "global_sentiment_polarity         0\n",
       "global_rate_positive_words        0\n",
       "global_rate_negative_words        0\n",
       "rate_positive_words               0\n",
       "rate_negative_words               0\n",
       "avg_positive_polarity             0\n",
       "min_positive_polarity             0\n",
       "max_positive_polarity             0\n",
       "avg_negative_polarity             0\n",
       "min_negative_polarity             0\n",
       "max_negative_polarity             0\n",
       "title_subjectivity                0\n",
       "title_sentiment_polarity          0\n",
       "abs_title_subjectivity            0\n",
       "abs_title_sentiment_polarity      0\n",
       "shares                            0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for Nulls\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of shares\n",
    "y = dataset.iloc[:,60].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:,1:60].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x = sc.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.022</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.021</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   12.68</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 17 Jul 2019</td> <th>  Prob (F-statistic):</th>  <td>1.39e-114</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:14:07</td>     <th>  Log-Likelihood:    </th> <td>-3.4228e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 31715</td>      <th>  AIC:               </th>  <td>6.847e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 31658</td>      <th>  BIC:               </th>  <td>6.852e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    57</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>  346.0862</td> <td>   96.077</td> <td>    3.602</td> <td> 0.000</td> <td>  157.771</td> <td>  534.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>  <td>  236.2730</td> <td>   70.521</td> <td>    3.350</td> <td> 0.001</td> <td>   98.048</td> <td>  374.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>  <td>  387.8900</td> <td>  119.467</td> <td>    3.247</td> <td> 0.001</td> <td>  153.730</td> <td>  622.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>  <td> 1.891e+04</td> <td> 7731.005</td> <td>    2.446</td> <td> 0.014</td> <td> 3759.694</td> <td> 3.41e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>  <td>-7105.7121</td> <td> 3.19e+04</td> <td>   -0.223</td> <td> 0.824</td> <td>-6.96e+04</td> <td> 5.54e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>  <td>-9793.2214</td> <td> 6081.628</td> <td>   -1.610</td> <td> 0.107</td> <td>-2.17e+04</td> <td> 2127.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>  <td>  213.0870</td> <td>   86.700</td> <td>    2.458</td> <td> 0.014</td> <td>   43.151</td> <td>  383.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>  <td> -262.5661</td> <td>   79.229</td> <td>   -3.314</td> <td> 0.001</td> <td> -417.858</td> <td> -107.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>  <td>   49.5978</td> <td>   84.932</td> <td>    0.584</td> <td> 0.559</td> <td> -116.873</td> <td>  216.068</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th> <td>   56.5373</td> <td>   73.687</td> <td>    0.767</td> <td> 0.443</td> <td>  -87.893</td> <td>  200.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th> <td> -441.9228</td> <td>  234.872</td> <td>   -1.882</td> <td> 0.060</td> <td> -902.281</td> <td>   18.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th> <td>  166.3950</td> <td>   81.231</td> <td>    2.048</td> <td> 0.041</td> <td>    7.179</td> <td>  325.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th> <td> -155.4173</td> <td>  100.906</td> <td>   -1.540</td> <td> 0.124</td> <td> -353.198</td> <td>   42.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th> <td> -378.7877</td> <td>  111.917</td> <td>   -3.385</td> <td> 0.001</td> <td> -598.150</td> <td> -159.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th> <td> -183.3941</td> <td>  159.077</td> <td>   -1.153</td> <td> 0.249</td> <td> -495.191</td> <td>  128.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th> <td>  -55.3619</td> <td>   99.799</td> <td>   -0.555</td> <td> 0.579</td> <td> -250.972</td> <td>  140.249</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th> <td>  -66.4511</td> <td>  164.938</td> <td>   -0.403</td> <td> 0.687</td> <td> -389.737</td> <td>  256.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th> <td>  -23.0593</td> <td>  176.137</td> <td>   -0.131</td> <td> 0.896</td> <td> -368.294</td> <td>  322.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th> <td>  137.9961</td> <td>  130.243</td> <td>    1.060</td> <td> 0.289</td> <td> -117.285</td> <td>  393.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th> <td>  246.5277</td> <td>  209.631</td> <td>    1.176</td> <td> 0.240</td> <td> -164.357</td> <td>  657.412</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th> <td> -142.5159</td> <td>  206.580</td> <td>   -0.690</td> <td> 0.490</td> <td> -547.420</td> <td>  262.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th> <td> -118.1467</td> <td>   78.589</td> <td>   -1.503</td> <td> 0.133</td> <td> -272.184</td> <td>   35.891</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th> <td>   -0.3272</td> <td>  145.070</td> <td>   -0.002</td> <td> 0.998</td> <td> -284.670</td> <td>  284.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th> <td>  -16.9939</td> <td>  131.056</td> <td>   -0.130</td> <td> 0.897</td> <td> -273.869</td> <td>  239.882</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th> <td> -341.7176</td> <td>   98.421</td> <td>   -3.472</td> <td> 0.001</td> <td> -534.627</td> <td> -148.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th> <td>-1205.4656</td> <td>  175.497</td> <td>   -6.869</td> <td> 0.000</td> <td>-1549.447</td> <td> -861.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th> <td> 2149.2045</td> <td>  215.679</td> <td>    9.965</td> <td> 0.000</td> <td> 1726.466</td> <td> 2571.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th> <td>  529.9049</td> <td>  163.393</td> <td>    3.243</td> <td> 0.001</td> <td>  209.648</td> <td>  850.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th> <td>  255.0362</td> <td>  198.043</td> <td>    1.288</td> <td> 0.198</td> <td> -133.136</td> <td>  643.208</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th> <td> -184.6444</td> <td>  285.717</td> <td>   -0.646</td> <td> 0.518</td> <td> -744.661</td> <td>  375.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th> <td>  160.4683</td> <td>   54.410</td> <td>    2.949</td> <td> 0.003</td> <td>   53.823</td> <td>  267.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th> <td>  -46.0750</td> <td>   53.350</td> <td>   -0.864</td> <td> 0.388</td> <td> -150.643</td> <td>   58.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th> <td>  -42.4799</td> <td>   53.494</td> <td>   -0.794</td> <td> 0.427</td> <td> -147.330</td> <td>   62.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th> <td>  -70.9449</td> <td>   53.637</td> <td>   -1.323</td> <td> 0.186</td> <td> -176.076</td> <td>   34.186</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th> <td>  -48.6955</td> <td>   55.865</td> <td>   -0.872</td> <td> 0.383</td> <td> -158.192</td> <td>   60.801</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th> <td>  110.1377</td> <td>   51.098</td> <td>    2.155</td> <td> 0.031</td> <td>    9.984</td> <td>  210.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th> <td>  -29.4656</td> <td>   49.762</td> <td>   -0.592</td> <td> 0.554</td> <td> -127.002</td> <td>   68.071</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th> <td>   56.5219</td> <td>   32.127</td> <td>    1.759</td> <td> 0.079</td> <td>   -6.449</td> <td>  119.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th> <td> 1.038e+05</td> <td> 1.66e+06</td> <td>    0.063</td> <td> 0.950</td> <td>-3.15e+06</td> <td> 3.36e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th> <td>  8.66e+04</td> <td> 1.39e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-2.63e+06</td> <td> 2.81e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th> <td>  1.11e+05</td> <td> 1.78e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-3.38e+06</td> <td>  3.6e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th> <td> 1.164e+05</td> <td> 1.86e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-3.54e+06</td> <td> 3.77e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th> <td>  1.14e+05</td> <td> 1.83e+06</td> <td>    0.062</td> <td> 0.950</td> <td>-3.47e+06</td> <td> 3.69e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th> <td>  254.7305</td> <td>  113.831</td> <td>    2.238</td> <td> 0.025</td> <td>   31.617</td> <td>  477.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th> <td>   47.5248</td> <td>  185.523</td> <td>    0.256</td> <td> 0.798</td> <td> -316.108</td> <td>  411.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th> <td> -343.4411</td> <td>  144.518</td> <td>   -2.376</td> <td> 0.017</td> <td> -626.703</td> <td>  -60.180</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th> <td>   -0.5963</td> <td>  172.370</td> <td>   -0.003</td> <td> 0.997</td> <td> -338.447</td> <td>  337.255</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th> <td>  303.4569</td> <td> 1127.012</td> <td>    0.269</td> <td> 0.788</td> <td>-1905.530</td> <td> 2512.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th> <td>  188.7744</td> <td>  934.501</td> <td>    0.202</td> <td> 0.840</td> <td>-1642.884</td> <td> 2020.433</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th> <td>  -59.4664</td> <td>  163.756</td> <td>   -0.363</td> <td> 0.717</td> <td> -380.434</td> <td>  261.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th> <td> -179.5387</td> <td>   93.724</td> <td>   -1.916</td> <td> 0.055</td> <td> -363.241</td> <td>    4.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th> <td>   64.9207</td> <td>  122.148</td> <td>    0.531</td> <td> 0.595</td> <td> -174.494</td> <td>  304.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th> <td> -269.0167</td> <td>  184.346</td> <td>   -1.459</td> <td> 0.144</td> <td> -630.343</td> <td>   92.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th> <td>   -0.9319</td> <td>  152.750</td> <td>   -0.006</td> <td> 0.995</td> <td> -300.327</td> <td>  298.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th> <td>   29.4675</td> <td>  114.019</td> <td>    0.258</td> <td> 0.796</td> <td> -194.014</td> <td>  252.949</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th> <td>   60.8415</td> <td>  101.432</td> <td>    0.600</td> <td> 0.549</td> <td> -137.970</td> <td>  259.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th> <td>  101.8147</td> <td>   75.955</td> <td>    1.340</td> <td> 0.180</td> <td>  -47.060</td> <td>  250.689</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th> <td>  144.9782</td> <td>   78.792</td> <td>    1.840</td> <td> 0.066</td> <td>   -9.456</td> <td>  299.413</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th> <td>   33.2960</td> <td>  102.193</td> <td>    0.326</td> <td> 0.745</td> <td> -167.007</td> <td>  233.599</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>84024.349</td> <th>  Durbin-Watson:     </th>    <td>   1.830</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>3206881176.823</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>31.536</td>   <th>  Prob(JB):          </th>    <td>    0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>1559.533</td>  <th>  Cond. No.          </th>    <td>5.07e+15</td>   \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 5.93e-27. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.022\n",
       "Model:                            OLS   Adj. R-squared:                  0.021\n",
       "Method:                 Least Squares   F-statistic:                     12.68\n",
       "Date:                Wed, 17 Jul 2019   Prob (F-statistic):          1.39e-114\n",
       "Time:                        13:14:07   Log-Likelihood:            -3.4228e+05\n",
       "No. Observations:               31715   AIC:                         6.847e+05\n",
       "Df Residuals:                   31658   BIC:                         6.852e+05\n",
       "Df Model:                          57                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1           346.0862     96.077      3.602      0.000     157.771     534.401\n",
       "x2           236.2730     70.521      3.350      0.001      98.048     374.497\n",
       "x3           387.8900    119.467      3.247      0.001     153.730     622.050\n",
       "x4          1.891e+04   7731.005      2.446      0.014    3759.694    3.41e+04\n",
       "x5         -7105.7121   3.19e+04     -0.223      0.824   -6.96e+04    5.54e+04\n",
       "x6         -9793.2214   6081.628     -1.610      0.107   -2.17e+04    2127.007\n",
       "x7           213.0870     86.700      2.458      0.014      43.151     383.023\n",
       "x8          -262.5661     79.229     -3.314      0.001    -417.858    -107.274\n",
       "x9            49.5978     84.932      0.584      0.559    -116.873     216.068\n",
       "x10           56.5373     73.687      0.767      0.443     -87.893     200.968\n",
       "x11         -441.9228    234.872     -1.882      0.060    -902.281      18.436\n",
       "x12          166.3950     81.231      2.048      0.041       7.179     325.611\n",
       "x13         -155.4173    100.906     -1.540      0.124    -353.198      42.363\n",
       "x14         -378.7877    111.917     -3.385      0.001    -598.150    -159.425\n",
       "x15         -183.3941    159.077     -1.153      0.249    -495.191     128.403\n",
       "x16          -55.3619     99.799     -0.555      0.579    -250.972     140.249\n",
       "x17          -66.4511    164.938     -0.403      0.687    -389.737     256.835\n",
       "x18          -23.0593    176.137     -0.131      0.896    -368.294     322.176\n",
       "x19          137.9961    130.243      1.060      0.289    -117.285     393.277\n",
       "x20          246.5277    209.631      1.176      0.240    -164.357     657.412\n",
       "x21         -142.5159    206.580     -0.690      0.490    -547.420     262.389\n",
       "x22         -118.1467     78.589     -1.503      0.133    -272.184      35.891\n",
       "x23           -0.3272    145.070     -0.002      0.998    -284.670     284.015\n",
       "x24          -16.9939    131.056     -0.130      0.897    -273.869     239.882\n",
       "x25         -341.7176     98.421     -3.472      0.001    -534.627    -148.808\n",
       "x26        -1205.4656    175.497     -6.869      0.000   -1549.447    -861.484\n",
       "x27         2149.2045    215.679      9.965      0.000    1726.466    2571.943\n",
       "x28          529.9049    163.393      3.243      0.001     209.648     850.162\n",
       "x29          255.0362    198.043      1.288      0.198    -133.136     643.208\n",
       "x30         -184.6444    285.717     -0.646      0.518    -744.661     375.372\n",
       "x31          160.4683     54.410      2.949      0.003      53.823     267.114\n",
       "x32          -46.0750     53.350     -0.864      0.388    -150.643      58.493\n",
       "x33          -42.4799     53.494     -0.794      0.427    -147.330      62.370\n",
       "x34          -70.9449     53.637     -1.323      0.186    -176.076      34.186\n",
       "x35          -48.6955     55.865     -0.872      0.383    -158.192      60.801\n",
       "x36          110.1377     51.098      2.155      0.031       9.984     210.291\n",
       "x37          -29.4656     49.762     -0.592      0.554    -127.002      68.071\n",
       "x38           56.5219     32.127      1.759      0.079      -6.449     119.492\n",
       "x39         1.038e+05   1.66e+06      0.063      0.950   -3.15e+06    3.36e+06\n",
       "x40          8.66e+04   1.39e+06      0.062      0.950   -2.63e+06    2.81e+06\n",
       "x41          1.11e+05   1.78e+06      0.062      0.950   -3.38e+06     3.6e+06\n",
       "x42         1.164e+05   1.86e+06      0.062      0.950   -3.54e+06    3.77e+06\n",
       "x43          1.14e+05   1.83e+06      0.062      0.950   -3.47e+06    3.69e+06\n",
       "x44          254.7305    113.831      2.238      0.025      31.617     477.844\n",
       "x45           47.5248    185.523      0.256      0.798    -316.108     411.158\n",
       "x46         -343.4411    144.518     -2.376      0.017    -626.703     -60.180\n",
       "x47           -0.5963    172.370     -0.003      0.997    -338.447     337.255\n",
       "x48          303.4569   1127.012      0.269      0.788   -1905.530    2512.444\n",
       "x49          188.7744    934.501      0.202      0.840   -1642.884    2020.433\n",
       "x50          -59.4664    163.756     -0.363      0.717    -380.434     261.501\n",
       "x51         -179.5387     93.724     -1.916      0.055    -363.241       4.163\n",
       "x52           64.9207    122.148      0.531      0.595    -174.494     304.335\n",
       "x53         -269.0167    184.346     -1.459      0.144    -630.343      92.309\n",
       "x54           -0.9319    152.750     -0.006      0.995    -300.327     298.463\n",
       "x55           29.4675    114.019      0.258      0.796    -194.014     252.949\n",
       "x56           60.8415    101.432      0.600      0.549    -137.970     259.653\n",
       "x57          101.8147     75.955      1.340      0.180     -47.060     250.689\n",
       "x58          144.9782     78.792      1.840      0.066      -9.456     299.413\n",
       "x59           33.2960    102.193      0.326      0.745    -167.007     233.599\n",
       "==============================================================================\n",
       "Omnibus:                    84024.349   Durbin-Watson:                   1.830\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):       3206881176.823\n",
       "Skew:                          31.536   Prob(JB):                         0.00\n",
       "Kurtosis:                    1559.533   Cond. No.                     5.07e+15\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 5.93e-27. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use stat to see and delete x-vars that have |p|>t<0.05\n",
    "import statsmodels.formula.api as sm\n",
    "lm2 = sm.OLS(y_train,x_train).fit()\n",
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new x valuesfrom sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "x = dataset.iloc[:,[1,2,3,7,8,11,14,20,25,26,27,28,29]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Reg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm = lm.fit(x_train,y_train)   #lm.fit(input,output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict \n",
    "y_pred = lm.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042534203626099276"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best r^2 score I got. You can use lm.predict to predict values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
